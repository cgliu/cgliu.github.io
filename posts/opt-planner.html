<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Optimal Control and Reinforcement Learning</title>
<meta name="generator" content="Org mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<!-- Optional theme -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">
<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.js"></script>
<script type="text/javascript" src="http://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>
<link rel="stylesheet" type="text/css" href="../css/theme-readtheorg-custom.css" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="./opt-planner.html"> UP </a>
 |
 <a accesskey="H" href="../index.html"> HOME </a>
</div><div id="content">
<h1 class="title">Optimal Control and Reinforcement Learning</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgd007fc8">Preface</a></li>
<li><a href="#orgbea1714">Introduction</a></li>
<li><a href="#org9af469e">Problem formulation (continuous time)</a>
<ul>
<li><a href="#orga02d397">Bolza Formulation</a></li>
<li><a href="#org497f2ff">Mayer Formulation</a></li>
<li><a href="#org4649744">Hamilton-Jacobi-Bellman equation (HJB) and Pontryagin Maximum Principle</a></li>
<li><a href="#org1ffe8d5">Euler-Lagrange Equation</a>
<ul>
<li><a href="#org0b5fd40">Short proof:</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org84db1d8">Lagrange Equations</a></li>
<li><a href="#org6c2583f">Hamilton Equations</a></li>
<li><a href="#org4fd51f8">Introduction</a></li>
<li><a href="#org98b6c25">Dynamic Programming</a></li>
<li><a href="#org4126e9d">Direct Method</a>
<ul>
<li><a href="#org3a4a6d9">Direct (Multiple) Shooting Method</a></li>
<li><a href="#orgf70e806">Direct Collocation Method</a></li>
<li><a href="#org3f68043">Illustration: One Pendulum Swing-up</a></li>
</ul>
</li>
<li><a href="#org0958cf4">Indirect Method</a>
<ul>
<li><a href="#org6016f0e">The Gradient Method</a></li>
<li><a href="#orgf22a8ba">The Newton-Raphson Method</a></li>
<li><a href="#orgb7945cc">Steady-state Local Models</a></li>
<li><a href="#orgd71c3e2">One-link Pendulum Swingup Example Problem</a></li>
</ul>
</li>
<li><a href="#orgd779ec9">Introduction of Optimal Control</a>
<ul>
<li><a href="#org0427c37">Infinite Time Horizon Optimal Control</a></li>
<li><a href="#org16c0f27">Solution via Dynamic Programming</a></li>
</ul>
</li>
<li><a href="#org74aa5a7">Introduction of Model Predictive Control</a>
<ul>
<li><a href="#org78034a2">Model Predictive Control (MPC)</a></li>
<li><a href="#orge7ad4e2">Variation on MPC</a></li>
<li><a href="#orgdc2f897">Variation on MPC (Continuation)</a></li>
</ul>
</li>
<li><a href="#org4d57b19">CHOMP (Co-variant Hamiltonian Optimization for Motion Planning)</a>
<ul>
<li><a href="#orgbb984bb">Remarks</a></li>
<li><a href="#org6ba2343">Problem formulation</a></li>
<li><a href="#orgab6d67e">Functional gradient</a></li>
<li><a href="#orgc74b8d3">Functional gradient</a></li>
<li><a href="#org54f3a29">Functionals and Co-variant Gradients for Trajectory Optimization</a></li>
<li><a href="#orga36400f">Waypoint Trajectory Parameterization</a></li>
<li><a href="#orgc95d3e1">Update rule:</a></li>
<li><a href="#org2f06655">Hamiltonian Monte Carlo</a></li>
</ul>
</li>
<li><a href="#org16d54f6">Optimal Smoothing and Its Application</a>
<ul>
<li><a href="#orga709b15">Optimal Smoothing Background</a>
<ul>
<li><a href="#orgd5e00db">Optimal smoothing</a></li>
<li><a href="#orgdcfccbb">Optimal linear quadratic smoother</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgd007fc8" class="outline-2">
<h2 id="orgd007fc8">Preface</h2>
<div class="outline-text-2" id="text-orgd007fc8">
<p>
This notes is about optimal control and trajectory optimization. Most of it's content are cited from books and
websites, I put them together in a more readable manner for myself and for all who are passionate about it..
</p>

<p>
I will use the following notations unless specified otherwise:
</p>
<ul class="org-ul">
<li>\(\mathbf{x} \in \mathbb{R}^n\): the vector of state variables</li>
<li>\(\mathbf{u} \in \mathbb{R}^m\): the vector of control variables</li>
<li>\(J(\mathbf{x},\mathbf{u},t_f)\): the objective function or performance
index</li>
<li>\(L(\mathbf{x},\mathbf{u})\): for discrete-time system, called one step cost
function; for continuous-time system, called Lagrangian.</li>
<li>\(V(\mathbf{x}) \in \mathbb{R}\): the value function or the optimal
cost function, or the optimal return function.
\(V(\mathbf{x},t) = \min_{\mathbf{u}(\cdot)}J(\mathbf{x},\mathbf{u},t)\)</li>
<li>\(R(\mathbf{x},\mathbf{u})\): the reward function</li>
</ul>
</div>
</div>

<div id="outline-container-orgbea1714" class="outline-2">
<h2 id="orgbea1714">Introduction</h2>
<div class="outline-text-2" id="text-orgbea1714">
<p>
Optimal control theory, an extension of the calculus of variations, is a mathematical optimization method for deriving
control policies. The method is largely due to the work of Lev Pontryagin and his collaborators in the Soviet Union
and Richard Bellman in the United States.
</p>

<p>
Optimal control deals with the problem of finding a control law for a given system such that a certain optimality
criterion is achieved. A control problem includes a cost functional (or called performance index) that is a function of state \(\mathbf{x} \in
  \mathbb{R}^n\) and control \(\mathbf{u} \in \mathbb{R}^m\) and/or the terminal time \(t_f\). Minimize the continuous-time cost
functional or discrete cost summation.
</p>

<p>
Three important elements:
</p>
<ol class="org-ol">
<li>dynamics model</li>
<li>cost (reward) function</li>
<li>policy or control law</li>
</ol>

<p>
For optimal control problem, in general 1) and 2) are known, the problem is to find an optimal 3); For Reinforcement
Learning problem, 1) and 2) are often unknown, they are measured from the physical world or simulation world, the goal
is the same, to find the optimal 3).
</p>
</div>
</div>

<div id="outline-container-org9af469e" class="outline-2">
<h2 id="org9af469e">Problem formulation (continuous time)</h2>
<div class="outline-text-2" id="text-org9af469e">
</div>
<div id="outline-container-orga02d397" class="outline-3">
<h3 id="orga02d397">Bolza Formulation</h3>
<div class="outline-text-3" id="text-orga02d397">
\begin{equation}
  \label{eq:b1}
  J(u(\cdot),t_f) = \phi(\mathbf{x}(t_f),t_f) + \int_{t_0}^{t_f}L(\mathbf{x},\mathbf{u},t)dt
\end{equation}
<p>
s.t.
</p>
\begin{equation}
  \label{eq:b2}
  \dot{\mathbf{x}} = f(\mathbf{x},\mathbf{u},t),
\end{equation}
<p>
and the boundary conditions
</p>
\begin{equation}
  \label{eq:10}
  \mathbf{x}(t_0)=\mathbf{x}_0,\quad \psi(\mathbf{x}_f,t_f) = 0
\end{equation}
<p>
where \(\mathbf{x}(t)\in \mathbb{R}^n\) is the state, \(\mathbf{u}(t)\in \mathbb{R}^m\) is the control, \(t\) is the independent
variable (generally speaking, time), \(t_0\) is the initial time, and \(t_f\) is the terminal time. The terms \(\phi\) and \(L\)
are called the terminal penalty term and Lagrangian, respectively.
</p>
</div>
</div>

<div id="outline-container-org497f2ff" class="outline-3">
<h3 id="org497f2ff">Mayer Formulation</h3>
<div class="outline-text-3" id="text-org497f2ff">
<p>
In the Mayer formulation, the sate vector is extended by one state
\(x_{n+1}(t)\).
</p>
\begin{equation}
  \label{eq:m1}
  x_{n+1}(t) = \int_{t_0}^tL(\mathbf{x},\mathbf{u},t)dt.
\end{equation}
<p>
Then the is to choose \(\mathbf{u}(t)\) to minimize
</p>
\begin{equation}
  \label{eq:m2}
  J(u(\cdot),t_f) = \phi(\mathbf{x}(t_f),t_f)
\end{equation}
<p>
s.t.
</p>
\begin{equation}
  \label{eq:m3}
  \dot{\mathbf{x}} = f(\mathbf{x},\mathbf{u},t),
\end{equation}
<p>
and the boundary conditions
</p>

<p>
\[
  \mathbf{x}(t_0) =\mathbf{x}_0,\quad \psi(\mathbf{x}_f,t_f) = 0
\]
</p>

<p>
Mayer formulation and Bolza formulation are equivalent, but Mayer form yield simpler expression.
</p>

<p>
in particular, there are constraints on the state of the form
</p>
\begin{equation}
  \label{eq:state-constraint}
  S(\mathbf{x}(t)) \le 0
\end{equation}
<p>
and on the control variable
</p>
\begin{equation}
  \label{eq:control-constraint}
  C(\mathbf{x}(t),\mathbf{u}(t)) \le 0
\end{equation}

<p>
The optimal control can be derived using Pontryagin's maximum principle (a necessary condition), or by solving the
Hamilton-Jacobi-Bellman equation (a sufficient condition).
</p>
</div>
</div>

<div id="outline-container-org4649744" class="outline-3">
<h3 id="org4649744">Hamilton-Jacobi-Bellman equation (HJB) and Pontryagin Maximum Principle</h3>
<div class="outline-text-3" id="text-org4649744">
<p>
The Hamilton-Jacobi-Bellman (HJB) equation is a partial differential equation which is central to optimal control
theory. The solution of the HJB equation is the <i>value function</i>, which gives the optimal cost-to-go for a given
dynamical system with an associated cost function. The solution is open loop, but it also permits the solution of the
closed loop problem. Classical variational problems, for example, the brachistochrone problem can be solved using this
method. The HJB method can be generalized to stochastic systems as well.
</p>

<p>
The equation is a result of the theory of dynamic programming which was pioneered in the 1950s by Richard Bellman and
coworkers. The corresponding discrete-time equation is usually referred to as the <i>Bellman equation</i>. In continuous
time, the result can be seen as an extension of earlier work in classical physics on the Hamilton-Jacobi equation by
William Rowan Hamilton and Carl Gustav Jacob Jacobi.
</p>

<p>
Intuitively, HJB can be derived as follows:
</p>
\begin{equation}
  \label{eq:4}
  V(\mathbf{x}(t),t) = \min_{\mathbf{u}} \{L(\mathbf{x}(t),\mathbf{u}(t),t)\dt+V(\mathbf{x}(t+\dt),t+\dt)\}
\end{equation}
<p>
The Taylor expansion of the last term is:
</p>
\begin{equation}
  \label{eq:5}
  V(\mathbf{x}(t+\dt),t+\dt) = V(\mathbf{x},t) + V_t\dt +
  V_{\mathbf{x}}\dot{\mathbf{x}}\dt + o(\dt^2)
\end{equation}
<p>
where \(o(\dt^2)\) denotes the terms in the Taylor expansion of higher order than one. Then if we cancel \(V(\mathbf{x}(t),t)\)
on both sides, divide by \(\dt\), and take the limit as \(\dt\) approaches zero, then we obtain the HJB equation as:
</p>

\begin{equation}
  \label{eq:HJB}
  V_{t}(\mathbf{x},t) + \min_{\mathbf{u}}\{V_{\mathbf{x}}f(\mathbf{x},\mathbf{u}) + L(\mathbf{x},\mathbf{u})\} = 0
\end{equation}
<p>
The HJB equation is a <b>necessary</b> and <b>sufficient</b> condition for an optimum. If we can solve for V then we can find from
it a control u that achieves the minimum cost.
</p>

<p>
Eq. \ref{eq:HJB} can be written as
</p>
\begin{eqnarray}
  \label{eq:6}
  -V_t(\mathbf{x},t) &=& \min_{\mathbf{u}} H(\mathbf{x},\mathbf{u},t,V_{\mathbf{x}}) \\
  H &:=& L(\mathbf{x},\mathbf{u})+V_{\mathbf{x}}f(\mathbf{x},\mathbf{u})
\end{eqnarray}
<p>
It shows that the optimal control \(\mathbf{u}^*\) is the value of \(\mathbf{u}\) that globally minimizes the Hamiltonian
\(H(\mathbf{x},\mathbf{u},t,V_{\mathbf{x}})\), holding \(\mathbf{x}\), \(V_{\mathbf{x}}\), and \(t\) constant. It was formulated by the Russian
mathematician Lev Semenovich Pontryagin and his students and known as <i>Pontryagin's minimum principle</i>.
</p>

<p>
One of the most effective way to solve HJB, which is a first-order nonlinear partial differential equation, is the
method of characteristic. It amounts to find a field of extremals. We can use Euler-Lagrange Equations, which is
ordinary differential equation (ODE), to solve a particular optimal path and its associate optimal function, thus get
the field of extremals.
</p>
</div>
</div>

<div id="outline-container-org1ffe8d5" class="outline-3">
<h3 id="org1ffe8d5">Euler-Lagrange Equation</h3>
<div class="outline-text-3" id="text-org1ffe8d5">
<p>
The <i>Euler-Lagrange equation</i> was developed in the 1750s by Euler and Lagrange in connection with their studies of the
\emph{tautochrone} problem. This is the problem of determining a curve on which a weighted particle will fall to a fixed
point in a fixed amount of time, independent of the starting point. Lagrange solved this problem in 1755 and sent the
solution to Euler. The two further developed <i>Lagrange's method</i> and applied it to mechanics, which led to the formulation
of <i>Lagrangian mechanics</i>. Their correspondence ultimately led to <i>the calculus of variations</i>, a term coined by
Euler himself in 1766.
</p>

<p>
In <i>calculus of variations</i>, the <i>Euler-Lagrange equation</i>, or <i>Lagrange's equation</i>, is a differential equation whose
solutions are the functions for which a given functional is stationary. Because a differentiable functional is
stationary at its local maxima and minima, the <i>Euler-Lagrange equation</i> is useful for solving optimization problems in
which, given some functional, one seeks the function minimizing (or maximizing) it.
</p>

<p>
The <i>Euler-Lagrange equation</i> is an equation satisfied by a function q of a real argument t which is a
stationary point of the functional
</p>
\begin{equation}
  \label{eq:26}
  S(q) = \int_{a}^{b}L(t,q,\dot{q})dt
\end{equation}

<p>
The <i>Euler-Lagrange equation</i>, then, is the ordinary differential equation
</p>
\begin{equation}
  \label{eq:28}
  \frac{\partial L}{\partial q}-\frac{d}{dt}\frac{\partial L}{\partial
  \dot{q}} = 0
\end{equation}

<p>
The solution of this eq. \ref{eq:28} is the stationary solution of eq. \ref{eq:26}[fn:prove].
</p>
</div>

<div id="outline-container-org0b5fd40" class="outline-4">
<h4 id="org0b5fd40">Short proof:</h4>
<div class="outline-text-4" id="text-org0b5fd40">
\begin{equation}
  \label{eq:11}
  \frac{\delta S}{\delta q} = 0
\end{equation}

\begin{eqnarray}
  \label{eq:33}
  \delta S(q(t)) &=& \int\frac{\partial L}{\partial q}\delta q +
  \frac{\partial{L}}{\partial{\dot{q}}}\delta{\dot{q}}\dt \\
  &=& \frac{\partial{L}}{\partial{\dot{q}}}\delta{q}(t)\Big|^{t_f}_{t_0} + \int
  \Big(\frac{\partial{L}}{\partial{q}} -
  \frac{d}{dt}\frac{\partial{L}}{\partial{\dot{q}}}\Big)\delta{q}\dt
\end{eqnarray}
<p>
The boundary conditions, \(\delta{q}(t_0) = 0\) and \(\delta{q}(t_f)=0\), causes the first term to vanish
</p>

<p>
<b>In physics</b>, <i>Hamilton's principle</i> is William Rowan Hamilton's formulation of the principle of stationary action. It
states that the dynamics of a physical system is determined by a variational problem for a functional based on a single
function, the Lagrangian, which contains all physical information concerning the system and the forces acting on it. The
evolution \(q(t)\) of a system described by \(q(t) \in \mathbb{R}^N\) between \((t_1,q(t_1))\) and \((t_2,q(t_2))\) is an
extremum of the action functional
</p>
\begin{equation}
  \label{eq:29}
  S(q):=\int_{t_1}^{t_2}L(t,q,\dot{q})dt
\end{equation}
<p>
where \(L(t,q,\dot{q}):= K-V\) is the Lagrangian function for the
system. <br />
A good tutorial can be found at: <br />
</p>
<ul class="org-ul">
<li><a href="http://www.eftaylor.com/software/ActionApplets/LeastAction.html">http://www.eftaylor.com/software/ActionApplets/LeastAction.html</a></li>
</ul>

<p>
<b>In Optimal Control</b>, the scalar Hamiltonian function \(H\) is defined as
</p>
\begin{equation}
  \label{eq:2}
  H[\mathbf{x}(t),\mathbf{u}(t),\lambda(t),t] := L(\mathbf{x},\mathbf{u},t) + \lambda^T(t) f(\mathbf{x},\mathbf{u},t),
\end{equation}
<p>
and we choose the multiplier function \(\lambda(t)\) to be
\(\dot{\lambda}^{T} =
-\frac{\partial{H}}{\partial{x}}=-L_{x}-\lambda^{T}f_x\), which is also
called co-state or adjoint variables, then
the continuous-time EL equation:
</p>
\begin{eqnarray}
  \label{eq:21}
  \dot{x} &=& \frac{\partial{H}}{\partial{\lambda^T}}=f(x,u,t) \\
  \dot{\lambda}^{T} &=& -\frac{\partial{H}}{\partial{x}}=-L_{x}-\lambda^{T}f_x\\
  H_u &=& L_{u}+\lambda^Tf_{u}0, \quad t_0 \le t \le t_f
\end{eqnarray}
<p>
with boundary conditions
</p>
\begin{eqnarray}
  \label{eq:22}
  x(t_0) \quad \textrm{if specified}, \\
  \lambda^{T}(t_f)=\phi_x(t_f)
\end{eqnarray}
<p>
The evolution of \(\lambda\) is given \emph{reverse time}, from a final state to the initial. Hence we see the primary
difficulty of solving optimal control problems: the state propagates forward in time, while the costate propogates
backward. The state and costate are coordinated through the above equations.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org84db1d8" class="outline-2">
<h2 id="org84db1d8">Lagrange Equations</h2>
<div class="outline-text-2" id="text-org84db1d8">
<p>
Lagrangian mechanics is a re-formulation of classical mechanics that
combines conservation of momentum with conservation of energy. It was
introduced by Italian mathematician Joseph-Louis Lagrange in 1788. In
Lagrangian mechanics, the trajectory of a system of particles is
derived by solving the Lagrange equations in one of two forms, named
the Lagrange equations of the first kind, which treat constraints
explicitly as extra equations, often using Lagrange multipliers;
and the Lagrange equations of the second kind, which incorporate the
constraints directly by judicious choice of generalized
coordinates. The fundamental lemma of the calculus of variations
shows that solving the Lagrange equations is equivalent to finding the
path for which the action functional is stationary, a quantity that is
the integral of the Lagrangian over time.
</p>

<p>
Define the following scalar function
</p>
\begin{equation}
  \label{eq:lagrange}
  L=K(\dot{q})-V(q),
\end{equation}
<p>
where \(K\) is kinetic energy and is a function of only \(\dot{q}\), \(V\)
is the potential energy and is a function of only \(q\), \(q\) is called
generalized coordinate. Lagrange equations are often written as
</p>
\begin{equation}
  \label{eq:lagrange-eqn}
  \frac{d}{dt}\frac{\partial L}{\partial \dot{q}} - \frac{\partial
    L}{\partial q} = 0.
\end{equation}

<p>
Intuitive example: <br />
</p>
\begin{equation}
  \label{eq:23}
  L = \frac{1}{2}mv^2-mgh
\end{equation}
<p>
where \(v:=\dot{q}\) and \(h:=z\),the
generalized coordinate is \(q:=(x,y,z)'\).
The first term of \ref{eq:lagrange-eqn}
</p>
\begin{equation}
  \label{eq:24}
  \frac{d}{dt}\frac{\partial L}{\partial \dot{q}} = m\ddot{q}.
\end{equation}
<p>
The second term is
</p>
\begin{equation}
  \label{eq:27}
  \frac{\partial L}{\partial q} = (0,0,mg)'
\end{equation}
<p>
According to Newton's second law, \(m\ddot{q}-(0,0,mg) = 0\).
</p>
</div>
</div>

<div id="outline-container-org6c2583f" class="outline-2">
<h2 id="org6c2583f">Hamilton Equations</h2>
<div class="outline-text-2" id="text-org6c2583f">
<p>
Hamiltonian mechanics is a reformulation of classical mechanics that
was introduced in 1833 by Irish mathematician William Rowan Hamilton.
It arose from Lagrangian mechanics, a previous reformulation of
classical mechanics introduced by Joseph Louis Lagrange in 1788, but
can be formulated without recourse to Lagrangian mechanics using
symplectic spaces (see Mathematical formalism, below). The Hamiltonian
method differs from the Lagrangian method in that instead of
expressing second-order differential constraints on an n-dimensional
coordinate space (where n is the number of degrees of freedom of the
system), it expresses first-order constraints on a 2n-dimensional
phase space.
</p>

\begin{eqnarray}
  \label{eq:hamilton-formula}
  \dot{p} &=& -\frac{\partial H}{\partial q} \\
  \dot{q} &=&  \frac{\partial H}{\partial p} \nonumber
\end{eqnarray}
<p>
where \(p(t)\) is generalized momenta, \(q(t)\) is generalized
coordinates, \(H\) is the Hamilton function, or Hamiltonian.
It is simply the total energy of the material point.
Intuitive Example: <br />
</p>
\begin{equation}
  \label{eq:25}
  H=\frac{p^2}{2m}+V(q)
\end{equation}
<p>
with the variable defined as:
</p>

<ul class="org-ul">
<li>\(p = m\dot{q}\): the momentum of the material point</li>
<li>\(q=(x,y,z)'\): the location of the material point</li>
<li>\(V(q)\): the potential energy</li>
</ul>

<p>
Recall that \(F=\dot{p}=-\frac{\partial V}{\partial q}\), then Eq.
\ref{eq:hamilton-formula} is true.
</p>

<p>
<b>Using Hamilton's equations</b>:
</p>

<ul class="org-ul">
<li>Write out the Lagrangian \(L=T - V\). Express \(T\) and \(V\) as though you were going to use Lagrange's equation.</li>
<li>Calculate the momenta by differentiating the Lagrangian with
respect to velocity, as \(p=\frac{\partial{L}}{\partial{\dot{q}}}\)</li>
<li>\(H=p\dot{q}-L\).</li>
</ul>
</div>
</div>



<div id="outline-container-org4fd51f8" class="outline-2">
<h2 id="org4fd51f8">Introduction</h2>
<div class="outline-text-2" id="text-org4fd51f8">
<p>
Generally speaking, optimal control problems are nonlinear and, thus do not have analytic solutions (e.g., like the
linear-quadratic optimal control problem). As a result, it is necessary to employ numerical methods to solve optimal
control problems. In the early years of optimal control (circa 1950s to 1980s) the favored approach for solving optimal
control problems was that of indirect methods. In an indirect method, the calculus of variations is employed to obtain
the first-order optimality conditions. These conditions result in a two-point (or, in the case of a complex problem, a
multi-point) boundary-value problem. This boundary-value problem actually has a special structure because it arises from
taking the derivative of an Hamiltonian. Thus, the resulting dynamical system is an Hamiltonian system of the form
</p>
\begin{eqnarray}
  \label{eq:el-state}
  \dot{x} &=& \frac{\partial{H}}{\partial{\lambda}^T} \\
  \label{eq:el-costate}
  \dot{\lambda}^T &=& -\frac{\partial{H}}{\partial{x}}
\end{eqnarray}
<p>
where
</p>
\begin{equation}
  \label{eq:14}
  H = L(x,u,t) + \lambda^Tf(x,u,t),
\end{equation}
<p>
is the augmented Hamiltonian and in an indirect method, the boundary-value problem is solved (using the appropriate
boundary or transversality conditions). The beauty of using an indirect method is that the state and adjoint state,
\(\lambda\), are solved for and the resulting solution is readily verified to be an extremal trajectory. The disadvantage
of indirect methods is that the boundary-value problem is often extremely difficult to solve (particularly for problems
that span large time intervals or problems with interior point constraints).
</p>

<p>
In direct methods, optimal control problems are transform into nonlinear programming
problems. By taking the sequence of control as decision variables and using explicit numerical integration, optimal
control problems become parametric optimization problem and can be solved by so-call direct shooting method
\cite{Hargraves1987}. The shooting methods require forward simulation of the dynamics to compute the cost function (and
its gradients), and therefore can be fairly expensive to evaluation. Moreover, it needs good starting trajectories. In
direct collocation \cite{Hargraves1987,VonStryk1993}, the state and/or control variables are approximated using an
appropriate function approximation (e.g., polynomial approximation or piecewise constant parameterization). Then, the
coefficients of the function approximations are treated as optimization variables and the problem becomes an nonlinear
programming problem (NLP), which can be solved by general nonlinear programming method, such as sequential quadratic
programming (SQP). Because of sparse characteristic, the NLP is easier to solve than the boundary-value problem.
Optimization software package, SNOPT, can be used to solve such large sparse NLPs \cite{Philip}. % As a result, the
range of problems that can be solved via direct % methods (particularly direct collocation methods which are very
popular these days) is % significantly larger than the range of problems that can be % solved via indirect methods
%\cite{Hargraves1987,VonStryk1993,VonStryk1992,Hardt1999}. Compared with indirect methods, this approach is found more
robust in term of finding solution when little knowledge is known about the final result. As a result, the range of
problems that can be solved via direct methods is larger than the range of problems that can be solved via indirect
methods \cite{Hargraves1987,VonStryk1993,VonStryk1992,Hardt1999}. But one disadvantage is that they produce less
accurate and sometimes physically incorrect results. Another disadvantage is local minima \cite{VonStryk1992}. In order
to improve the low accuracy of direct method results and to increase the convergence areas of indirect methods,
combination methods were proposed in \cite{VonStryk1992,Hardt1999}.
</p>
</div>
</div>

<div id="outline-container-org98b6c25" class="outline-2">
<h2 id="org98b6c25">Dynamic Programming</h2>
<div class="outline-text-2" id="text-org98b6c25">
<p>
A typical algorithm, such as Dynamic Programming (DP), often discretizes the states and controls into cells, refines a
candidate optimal cost function and/or a corresponding control policy function iteratively \cite{Larson1968}.
</p>

<p>
Rangom Sampling of State in Dynamic Programming, which combine sparse random sampling of states with local models and
local optimization, can solve some harder problem \cite{chris-smc08}.
</p>
\begin{figure}
  \centering
  \includegraphics[width=3in]{fig/random_sampling}
  \caption{(Left) example of a local approximation of a 1-D value function using
three quadratic models. (Right) (dots) random states used to plan one-link
swingup superimposed on a contour map of the value function. (Black lines)
optimized trajectories are shown starting from the random states.}
  \label{fig:ddp-opt-proc}
\end{figure}
</div>
</div>


<div id="outline-container-org4126e9d" class="outline-2">
<h2 id="org4126e9d">Direct Method</h2>
<div class="outline-text-2" id="text-org4126e9d">
<p>
The approach that has risen to prominence in numerical
optimal control over the past two decades (i.e., from the
1980s to the present) is that of so called direct methods.
In a direct method, the state and/or control are
approximated using an appropriate function approximation
(e.g., polynomial approximation). Simultaneously, the cost
functional is approximated as a cost function. Then, the
coefficients of the function approximations are treated as
optimization variables and the problem is "transcribed" to a
nonlinear optimization problem.
</p>
</div>

<div id="outline-container-org3a4a6d9" class="outline-3">
<h3 id="org3a4a6d9">Direct (Multiple) Shooting Method</h3>
<div class="outline-text-3" id="text-org3a4a6d9">
<p>
In numerical analysis, direct shooting method is the method
for solving a boundary value problem by reducing it to the
solution of an initial value problem.
</p>

<p>
Intuitively speaking, we want to shoot some target, \(p(t_f)=(x_f,y_f)\), from our
current position, \(p(0)=(x_0,y_0)\).  it is a two-point boundary problem.
We can solve the orbit of the bullet, \(p(t)\), if we
can find a appropriate initial shooting velocity \(\dot{p}(0)\).
So we convert this boundary value problem into the problem of finding the initial
velocity problem. This is why it is called 'shooting method'.
</p>

<p>
The prescribed boundary conditions for the problem are initial values of
the state \(\mathbf{x}(0)\) and final values of the costate \(\lambda^T(t_f)\). The traditional shooting
method integrates the state equations beginning at \(\mathbf{x}(0)\) and
from a guess for \(\lambda^T(t_0)\) for a pre-specified time \(t_f\).
When the numerical value obtained for \(\lambda^T(t_f)\) from the
integration does not match the boundary condition \(\lambda^T(t_f) = \phi_{x}(t_f)\)
, a zero-finding problem may be set up in terms of the constraints,
and Newton's method may then
be used to iterate on \(\lambda^T(t_0)\) until the boundary condition is satisfied.
</p>

<p>
<b>Multiple Shooting Method</b>: This procedure consists of dividing up the time interval \([0,t_f]\) into a
series of grid points \([0, t_1, t_2,\ldots, t_f]\), and a shooting method is performed between
successive grid points (see Figure
\ref{fig:multiple-shooting-method}).
A set of starting values for the state and the co-state
is required at each grid point in time, and continuity conditions for
the solution trajectory introduce additional interior boundary conditions which are
then incorporated into one large zero-finding problem to be solved.
</p>

<p>
Advantages and disadvantages:
</p>


<ul class="org-ul">
<li>The strong sensitivity of initial value problems, and the small convergence</li>
</ul>
<p>
region for the Newton's method make solving these problems via simple shooting
very difficult.
</p>
<ul class="org-ul">
<li>Shooting methods require forward simulation of the dynamics to compute</li>
</ul>
<p>
the cost function ( and its gradients), and therefore can be fairly
expensive to evaluation.
</p>
<ul class="org-ul">
<li>Additionally, they do not make very effective</li>
</ul>
<p>
use of the capabilities of modern nonlinear optimization routine (like
SNOPT) to enforce constrains.
</p>


\begin{figure}
  \centering
  \includegraphics[width=2in]{fig/multiple-shooting.eps}
  \caption{Multiple Shooting Method}
  \label{fig:multiple-shooting-method}
\end{figure}
</div>
</div>

<div id="outline-container-orgf70e806" class="outline-3">
<h3 id="orgf70e806">Direct Collocation Method</h3>
<div class="outline-text-3" id="text-orgf70e806">
<p>
In direct collocation \cite{Hargraves1987,VonStryk1993},
the state and/or control variables are approximated using an appropriate function approximation
(e.g., polynomial approximation or piecewise constant
parameterization). Then, the coefficients of the function approximations are treated as
optimization variables and the problem becomes an nonlinear programming
problem (NLP), which can be solved by general nonlinear
programming method, such as sequential quadratic programming (SQP).
Because of sparse characteristic, the NLP is easier to solve than the
boundary-value problem.
Optimization software package, SNOPT, can be used to
solve such large sparse NLPs \cite{Philip}.
Compared with indirect methods, this approach is found
more robust in term of finding solution when little knowledge is known
about the final result.
As a result, the range of problems that can be solved via direct
methods is larger than the range of problems that can be
solved via indirect methods \cite{Hargraves1987,VonStryk1993,VonStryk1992,Hardt1999}.
But one disadvantage is that they produce less
accurate and sometimes physically incorrect results.
Another disadvantage is local minima \cite{VonStryk1992}.
</p>

<p>
<b>Midpoint Discretization Method</b>: <br />
The system dynamics in state space is
</p>

\begin{equation}
  \label{eq:13}
  \dot{\mathbf{x}} :=\frac{d}{\dt}
  \begin{bmatrix}
    p(t)\\
    v(t)
  \end{bmatrix} =
  \begin{bmatrix}
    v(t) \\
    acc(p(t),v(t),\mathbf{u}(t))
  \end{bmatrix}
\end{equation}
<p>
The discrete approximations to the velocity and the acceleration at each
interval are as follows:
</p>
\begin{eqnarray}
  \label{eq:6}
  v(i) &=& \frac{p(i+1) - p(i-1)}{2\textrm{dT}} \quad i\in [1,N-1]\\
  a(i) &=& \frac{p(i+1)+p(i-1) - 2p(i)}{\textrm{dT}^2}.
\end{eqnarray}
<p>
which are called \emph{midpoint discretization}.
The approximation to the end velocity is
</p>
\begin{equation}
  \label{eq:9}
  v(N) = v(N-1) + a(N-1)\textrm{dT}.
\end{equation}
\begin{equation}
  \label{eq:7}
  \mathbf{x}(i) := (p^T(i),v^T(i))^T \quad i \in [1,N]
\end{equation}
<p>
The performance index is then
</p>
\begin{equation}
  \label{eq:7}
  J =  \phi(\mathbf{x}(N)) + \sum_{i=1}^{N-1}L(\mathbf{x}(i),\mathbf{u}(i))\textrm{dT}
\end{equation}
<p>
s.t.
</p>
\begin{eqnarray}
  \label{eq:8}
  a(i) &=&acc(\mathbf{x}(i),\mathbf{u}(i))\\
  p(1) &=& p_0 \\
  v(1) &=& v_0\\
  p(N) &=& p_f \\
  v(N) &=& v_f \\
  lb_q \le &p(j)& \le ub_q \quad j\in[0,N]\\
  lb_v \le &v(k)& \le ub_v \quad k\in[1,N]\\
  lb_u \le &u(i)& \le ub_u \quad i = [1,N-1]
\end{eqnarray}
<p>
Parameters to be optimized  are
\([p(0),p(1),\ldots,p(N),u(1),u(1),\ldots,u(N-1)]\).
</p>

\begin{figure}%[htp]
  \centering
  \subfigure[Ringing]
  {
    \label{fig:ringing-exp}
    \includegraphics[width=1.5in]{fig/ringing}}
  \hspace{.1in}
  \subfigure[Smoothing]
  {
    \label{fig:smoothing-exp}
    \includegraphics[width=1.5in]{fig/ringing_smoothing}}
  \caption{Results of Midpoint Discretization Method.}
  \label{fig:one-link-exp}
\end{figure}

<p>
<b>Piece-wise Constant Integration Method</b>:
This method over-parameterizes the system with both
\(\mathbf{x}\) and \(\mathbf{u}\) are explicit decision variables as
\([\mathbf{x}(0),\mathbf{x}(1),\ldots,\mathbf{x}(N),\mathbf{u}(0),\mathbf{u}(1),\ldots,\mathbf{u}(N-1)]\).
The control are chosen as piece-wise constant between \(\mathbf{u}(k)\) and \(\mathbf{u}(k+1)\).
Besides constraints on state and constraints on control, the discrete
time dynamics equations are also imposed as
constraints on the state variables.
</p>

<p>
``Ringing'' Problem:
As shown in Fig. \ref{fig:ringing-exp},
this method sometimes has the problem of
``Ringing'' \cite{Vanderbei2001}.
</p>

<p>
<b>Smoothing</b>.
The model can be improved by adding to the objective some penalty on
the changes of the control variables, such as first-order smoothness
\(k\sum_{1}^{N-2}\big(\mathbf{u}(i+1)-\mathbf{u}(i)\big)^2\)
and second-order smoothness as \(k\sum_{2}^{N-2}\big(\mathbf{u}(i+1)+\mathbf{u}(i-1)-2\mathbf{u}(i)\big)^2\)
</p>


<p>
Direct methods are very popular today. The update is fast, and it is
very easy to implement constrains. A less obvious attribute of these
methods is that they may be
easier to initialize with trajectories that are in the vicinity of the
desired minima. The most commonly stated limitation of these methods is their accuracy.
</p>
</div>
</div>

<div id="outline-container-org3f68043" class="outline-3">
<h3 id="org3f68043">Illustration: One Pendulum Swing-up</h3>
<div class="outline-text-3" id="text-org3f68043">
\begin{figure}
  \centering
  \includegraphics[width=3.5in]{fig/swingup}
  \caption{One-link pendulum swing-up problem.
  The frames are taken in intervals of $0.5$ second.}
  \label{fig:pendulum-swing-up}
\end{figure}

<p>
We use one-link pendulum swingup as an intuitive example.
In one-link pendulum swingup, a motor at the base of the pendulum
swings a rigid arm from the downward stable equilibrium to the upright unstable equilibrium and
balances the arm there.
What makes this challenging is that the one-step cost function penalizes the amount of torque
used and the deviation of the current position from the goal.
The controller must try to minimize the total cost of the trajectory.
The performance index as
</p>
\begin{equation}
  \label{eq:12}
  J=\sum_{i=0}^{N-1}L(p(i),u(i))\mathrm{dT}
\end{equation}
<p>
where \(L = 0.1(p(i)-\pi)^2+u(i)^2\), \(p\) is the position, \(u\) is the applied torque, \(\mathrm{dT}\) is the
discretization time.
The optimization results are shown in \ref{fig:one-link-opt}.
</p>

\begin{figure}%[htp]
  \centering
  \subfigure[Local minimum results]
  {
    \label{fig:local-min}
    \includegraphics[width=1.5in]{fig/subopt-traj-2d}}
  \hspace{.1in}
  \subfigure[Global minimum results]
  {
    \label{fig:global-min}
    \includegraphics[width=1.5in]{fig/opt-traj-2d}}
  \subfigure[Optimal trajectories over the contour of value function
  of local minimum.]
  {
    \label{fig:subopt-traj-contour}
    \includegraphics[width=1.5in]{fig/subopt-traj-contour}}
  \subfigure[Optimal trajectories over the contour of value function of global minimum.]
  {
    \label{fig:opt-traj-contour}
    \includegraphics[width=1.5in]{fig/opt-traj-contour}}
  \caption{Trajectory optimization of one-link pendulum swing-up
    problem.}
  \label{fig:one-link-opt}
\end{figure}


<p>
\emph{In order to try to avoid local minima, information should be exchanged
between trajectories to enable convergence to globally optimal
trajectories.}
</p>

<p>
Some explain goes here &#x2026;
</p>

\begin{algorithm}
\caption{Trajectory optimization algorithm}
  \For{$level=0$ to $MaxCost$, $step$}{
    \If{$level <= Cost(T(i,j)) < level+step$}{
      Mark $(i,j)$\;
      $N++$\;
    }
    \While{$N > 0$}{
      \ForAll{$(i,j)$'s neighbors $(i',j')$}{
        Re-optimize trajectory $T(i',j')$ by using trajectory $T(i,j)$ as initial guess\;
        \If{ $Cost(T(i',j'))$ is reduced}{
          Mark $(i',j')$\;
          $N++$\;
        }
      }
      Un-mark $(i,j)$\;
      $N--$\;
    }
  }
\end{algorithm}
</div>
</div>
</div>

<div id="outline-container-org0958cf4" class="outline-2">
<h2 id="org0958cf4">Indirect Method</h2>
<div class="outline-text-2" id="text-org0958cf4">
</div>
<div id="outline-container-org6016f0e" class="outline-3">
<h3 id="org6016f0e">The Gradient Method</h3>
<div class="outline-text-3" id="text-org6016f0e">
<ul class="org-ul">
<li>Newton's method</li>
<li>gradient descent method</li>
<li>conjugate method</li>
<li>steep descent</li>
<li>multiple shooting method</li>
</ul>

<p>
EL summery:
</p>
\begin{eqnarray}
  \label{eq:difference-dyn1}
  \dot{\mathbf{x}} &=& f(\mathbf{x},\mathbf{u}) \\
  \dot{\lambda}^T &=& = -L_x - \lambda^Tf_x \\
  L_u+\lambda^Tf_u &=& 0 \\
  \lambda^T(t_f)&=& \phi_x(\mathbf{x}(t_f))
\end{eqnarray}
<p>
The gradient method is quite popular in applications and for
complex problems may be the only way to effectively develop
a solution.
</p>


<ul class="org-ul">
<li>For a given \(\mathbf{x}_0\), pick a control history \(\mathbf{u}(t)\).</li>
<li>Propagate \(\dot{\mathbf{x}} = f(\mathbf{x}, \mathbf{u}, t)\) forward in time to create a state trajectory.</li>
<li>Evaluate \(\phi(\mathbf{x}(tf))\), and the propagate costate
\(\lambda\) backward in time from \(t_f\) to \(t_0\), using Eq. \ref{eq:el-costate}.</li>
<li>At each time step, choose \(\mathbf{u} = -K(L_u + \lambda^Tf_u)\),
where \(K\) is a positive scalar or a positive definite matrix in the case of multiple input channels.</li>
<li>Let \(\mathbf{u} = \mathbf{u} + \delta \mathbf{u}\).</li>
<li>Go back to step 2 and repeat loop until solution has converged.</li>
</ul>


<p>
The major difficulties are computational expense, and the requirement of having a reasonable control
trajectory to begin.
</p>
</div>
</div>

<div id="outline-container-orgf22a8ba" class="outline-3">
<h3 id="orgf22a8ba">The Newton-Raphson Method</h3>
<div class="outline-text-3" id="text-orgf22a8ba">
<p>
The indirect method we use is called Difference Dynamic
Programming (DDP), which is
a second order local trajectory optimization method.
In stead of minimizing a cost function and calculate its gradients
directly, it take advantage of the
sequential forms of the discrete-time dynamic equation
\eqref{eq:difference-dyn1}, and
a local second-order expansion of the optimum cost function:
</p>
\begin{equation}
  \label{eq:value_approx}
  V(\mathbf{x}) \approx V_0 +
  V_x\mathbf{\delta{x}}+
  \frac{1}{2}\delta\mathbf{x}^{T}V_{xx}\delta\mathbf{{x}}
\end{equation}
<p>
where \(\delta\mathbf{x} = \mathbf{x}-\mathbf{x}^{j}\),
a local second-order expansion of the dynamics:
</p>
\begin{equation}
  \label{eq:dyn_approx}
  F(\mathbf{x},\mathbf{u}) \approx F_0 +
  F_{x}\mathbf{\delta{x}} + F_{u}\mathbf{\delta{u}} +
  \frac{1}{2}
  [\delta\mathbf{x}^T\,\delta\mathbf{u}^T]
  \begin{bmatrix}
    F_{xx}&F_{xu}\\
    F_{ux}&F_{uu}
  \end{bmatrix}
  \begin{bmatrix}
    \delta\mathbf{x}\\
    \delta\mathbf{u}
  \end{bmatrix}
\end{equation}
<p>
where \(\mathbf{\delta{u}} = \mathbf{u}-\mathbf{u}^{j}\), and
a local second-order expansion of one step cost function:
</p>
\begin{equation}
  \label{eq:one_step_cost_approx}
  L(\mathbf{x},\mathbf{u}) \approx L_0 + L_{x}\mathbf{\delta{x}} +L_{u}\mathbf{\delta{u}} +
  \frac{1}{2}
  [\delta\mathbf{x}^T\,\delta\mathbf{u}^T]
  \begin{bmatrix}
    L_{xx}&L_{xu}\\
    L_{ux}&L_{uu}
  \end{bmatrix}
  \begin{bmatrix}
    \delta\mathbf{x}\\
    \delta\mathbf{u}
  \end{bmatrix}.
\end{equation}
<p>
Given a starting trajectory \(\mathbf{x}^{j}\), one can integrate
the first and second partial derivatives of the cost function
backward in time by:
</p>
\begin{eqnarray}
  \label{eq:second_order_swap}
  %different from Dyer
  % P69!
  Z_x  &=& V_xF_x + L_x; \, Z_u = V_xF_u + L_u \\
  Z_{xx} &=& F^T_x V_{xx}F_x +
  V_xF_{xx} + L_{xx} \\
  Z_{xu} &=& F^{T}_xV_{xx}F_u +
  V_xF_{xu}+L_{xu} \\
  Z_{uu} &=& F^{T}_uV_{xx}F_u +
  V_xF_{uu} +L_{uu}  \\
  V_{x(k-1)} &=& Z_x- Z_uZ^{-1}_{uu}Z_{ux} \\
  V_{xx(k-1)} &=& Z_{xx} - Z_{xu}Z^{-1}_{uu}Z_{ux}
\end{eqnarray}
<p>
where the initial conditions are
</p>
\begin{eqnarray}
  \label{eq:value_boundary_condition}
  V_{x} = \phi_{x}(N) \\
  V_{xx} = \phi_{xx}(N)
\end{eqnarray}
<p>
and \(\phi_{x}\) and \(\phi_{xx}\) are the first and second partial
derivatives of terminal penalty function \(\phi\) with respect to \(\mathbf{x}(N)\).
A new updated trajectory \(\mathbf{x}^{j+1}(k)\) is generated by
integrating Eq. \eqref{eq:difference-dyn1} forward in time using the linear feedback law,
i.e.,
</p>
\begin{eqnarray}
  \label{eq:integrate_second_order}
  \mathbf{u}^{j+1} = \mathbf{u}^{j} - Z^{-1}_{uu}Z_{ux}[\mathbf{x}^{j+1}-\mathbf{x}^{j}] -
  Z^{-1}_{uu}Z_{u}.
\end{eqnarray}
<p>
Repeating these processes until the convergence to optimal state trajectory
\(\mathbf{x}^{o}\) and optimal control trajectory \(\mathbf{u}^{o}\) is achieved.
Constraints on state and control and on control can be handled by
adding soft penalty to the one step cost function.
</p>

\begin{figure}
  \centering
  \includegraphics[width=3in]{fig/traj_opt_ddp}
  \caption{The convergence to the optimal trajectories.}
  \label{fig:ddp-opt-proc}
\end{figure}
</div>
</div>


<div id="outline-container-orgb7945cc" class="outline-3">
<h3 id="orgb7945cc">Steady-state Local Models</h3>
<div class="outline-text-3" id="text-orgb7945cc">
<p>
A by-product of DDP is local models of optimal control law in the
neighborhood of the optimal trajectory,
\(\mathbf{K}(k) \triangleq \mathbf{u}^o_{\mathbf{x}}\), where
</p>
\begin{equation}
  \label{eq:noc-time-index}
  \mathbf{u}(k) = \mathbf{u}^o(k) + \mathbf{K}(k)[\mathbf{x}(k)-\mathbf{x}^o(k)]
\end{equation}
<p>
and \(\mathbf{K}(k) = -Z^-_{uu}(k)Z_{ux}(k)\).
For fixed-end time problem, we can calculate and store the optimal
control and state vectors \(\mathbf{u}^{o}(T)\) and \(\mathbf{x}^{o}(T)\) and
as the same time the optimal gain matrix \(\mathbf{K}(T)\), where
\(T=N-k=\textrm{step-to-go}\), then \(\mathbf{u}(T)\) can be calculated by
Eq. \eqref{eq:noc-time-index} \cite{Bryson1968}.
One drawback is that this feedback control law is time-variant.
In \cite{Tassa2008}, a fixed-length Receding Horizon scheme was
proposed to synthesize a time invariant controller
from many local controllers. In the spirit of Model Predictive Control \cite{72069}, we calculate
time-invariant or steady-state local models of the optimal cost function and the control law by designing
an appropriate terminal penalty term.
</p>

<p>
For a discrete-time dynamic system, the optimal cost function of
infinite temporal horizon
can be split up into two parts
</p>
\begin{equation}
  \label{eq:inifinite-horizon-cost-split}
  V^{\infty}\big(\mathbf{x}_0\big) =
  \min_{\mathbf{u}(\cdot)}
  \Big(\sum_{k=0}^{N-1}L\big(\mathbf{x}(k),\mathbf{u}(k)\big) +
  \sum_{k=N}^{\infty}L\big(\mathbf{x}(k),\mathbf{u}(k)\big)\Big).
\end{equation}
<p>
The goal is to approximate the second term on the right-hand side
by a terminal penalty term, \(\phi(\mathbf{x}(N))\).
Without further restrictions, it is difficult for general nonlinear
systems. However, if we ensure that the trajectories of the closed-loop system
remain within the vicinity of a goal state (terminal region) for
\(k\in[N,\infty)\), then the upper bound approximation to the second term can be found.
One solution is to determine the terminal region \(\mathcal{R}\) such
that a local state feedback law \(\mathbf{u}=\pi(\mathbf{x})\) asymptotically
stabilizes the nonlinear system and render \(\mathcal{R}\) positively
invariant under the closed-loop control.
If \(\mathbf{x}(N) \in \mathcal{R}\), then the second term of Eq. \eqref{eq:inifinite-horizon-cost-split}
can be upper bounded by the cost resulting from the application of
this local controller \(\mathbf{u}=\pi(\mathbf{x})\).
Requiring that \(\mathbf{x}(N) \in \mathcal{R}\) and using the local
controller \(\mathbf{u}=\pi(\mathbf{x})\) for \(k\in[N,\infty)\), we obtain:
</p>
\begin{equation}
  \label{eq:infinite-horizon-cost-split-2}
  V^{\infty}(\mathbf{x}_0) \approx
  \min_{\mathbf{u}(\cdot)}
  \Big(\sum_{k=0}^{N-1}L\big(\mathbf{x}(k),\mathbf{u}(k)\big) +
  \sum_{k=N}^{\infty}L\big(\mathbf{x}(k),\pi(\mathbf{x}(k))\big)\Big)
\end{equation}
<p>
If one step cost function \(L\) is with the form of
</p>
\begin{equation}
  \label{eq:quadratic-one-step-cost}
  L(\mathbf{x},\mathbf{u}) =\frac{1}{2} (\mathbf{x}-\mathbf{x}_d)^T\mathbf{Q}(\mathbf{x}-\mathbf{x}_d)
  + \frac{1}{2}(\mathbf{u}-\mathbf{u}_d)^T\mathbf{R}(\mathbf{u}-\mathbf{u}_d),
\end{equation}
<p>
a locally linear feedback law
\(\mathbf{u}=\mathbf{K}\mathbf{x}\) can be used and the terminal penalty term
can be approximated with a quadratic form
</p>
\begin{equation}
  \label{eq:terminal-cost}
  \phi(\mathbf{x}(N)) = \frac{1}{2}(\mathbf{x}(N)-\mathbf{x}_d)^T\mathbf{P}_{ss}(\mathbf{x}(N)-\mathbf{x}_d),
\end{equation}
<p>
\(\mathbf{P}_{ss}\) is given by solving the following matrix Riccati equation,
</p>
\begin{equation}
  \label{eq:riccati}
  \mathbf{P}_{ss}\mathbf{A} + \mathbf{A}^{T}\mathbf{P}_{ss} + \mathbf{Q} -
  \mathbf{P}_{ss}\mathbf{B}\mathbf{R}^{-1}\mathbf{B}^{T}\mathbf{P}_{ss} = 0,
\end{equation}
<p>
where \(\mathbf{A}=F_{x}\) and \(\mathbf{B} = F_{u}\) are the partial derivatives of \(F\)
with respect to desired state \(\mathbf{x}_d\) and corresponding control \(\mathbf{u}_d\).
Substituting Eq. \eqref{eq:terminal-cost} into Eq. \eqref{eq:infinite-horizon-cost-split-2} we obtain
</p>
\begin{equation}
  \label{eq:3}
  V^{\infty}(\mathbf{x}_0) \approx
  \min_{\mathbf{u}(\cdot)}
  \Big(
  \phi(\mathbf{x}(N)) + \sum_{k=0}^{N-1}L\big(\mathbf{x}(k),\mathbf{u}(k)\big)
  \Big).
\end{equation}
<p>
Requiring \(\mathbf{x}(N) \in \mathcal{R}\) and using
Eq. \eqref{eq:terminal-cost} as the terminal penalty term in Eq. \eqref{eq:b1},
the infinite temporal horizon optimal control can be upper approximated.
</p>

<p>
In general, only one optimal trajectory to the terminal manifold will
pass through a given state \((\mathbf{x},k)\),
so that a unique optimal control vector \(\mathbf{u}(\mathbf{x},k)\) is
associated with each state.
Furthermore, by using Eq. \eqref{eq:terminal-cost} as the terminal penalty term and
requiring \(\mathbf{x}(t_f)\in \mathcal{R}\),
we get steady-state linear models of the optimal control along the
optimal trajectory \(x^{o}\) as
</p>
\begin{equation}
  \label{eq:steady-state-linear-model}
  m_i(\mathbf{x}) := \mathbf{u}(i)+\mathbf{K}(i)(\mathbf{x}-\mathbf{x}(i)) \quad i \in [1,N]
\end{equation}
<p>
The control law can then be modeled by means of these spatially localized linear models.
A simple approach is using the closest local model to calculate control,
\(\mathbf{u}(\mathbf{x}) = m_{i}(\mathbf{x})\), where
\(||\mathbf{x}(i) - \mathbf{x}||_{D} \leq ||\mathbf{x}(j) - \mathbf{x}||_D\)
for all \(j \ne i\) and \(D\) is distance matrix.
</p>
</div>
</div>


<div id="outline-container-orgd71c3e2" class="outline-3">
<h3 id="orgd71c3e2">One-link Pendulum Swingup Example Problem</h3>
<div class="outline-text-3" id="text-orgd71c3e2">
\begin{figure}%[htp]
  \centering
  \subfigure[The optimal cost function.]
  {
    \label{fig:optimal-cost-tube}
    \includegraphics[width=1.5in]{fig/optimal_cost_tube}}
  \hspace{.1in}
  \subfigure[The optimal control policy.]
  {
    \label{fig:optimal-cost-tube}
    \includegraphics[width=1.5in]{fig/optimal_control_tube}}
  \caption{The optimal cost function and the optimal control policy for a one-link pendulum
    swingup problem. One optimal trajectory is shown as a white line.
    Red lines in both plots show the boundary of its neighborhood. Black circles
    are states used to construct local models of the optimal cost function in the left
    plot and of the control policy in the right plot.
    The optimal cost function is cut
    off above 20. The goal is at the state ($\pi$, 0)}
  \label{fig:one-link-exp}
\end{figure}
\begin{figure}%[htp]
  \centering
  \subfigure[Control policy using local models.]
  {
    \label{fig:optimal-cost-tube}
    \includegraphics[width=1.5in]{fig/NOC_policy}}
  \hspace{.1in}
  \subfigure[Value function of the control policy using local models]
  {
    \label{fig:optimal-cost-tube}
    \includegraphics[width=1.5in]{fig/NOC_value}}

  \subfigure[Optimal control policy]
  {
    \label{fig:optimal-cost-tube}
    \includegraphics[width=1.5in]{fig/optimal_control_policy}}
  \hspace{.1in}
  \subfigure[Value function of the optimal control policy]
  {
    \label{fig:optimal-cost-tube}
    \includegraphics[width=1.5in]{fig/optimal_control_value}}

  \caption{}
  \label{fig:one-link-exp}
\end{figure}

<p>
Fig. \ref{fig:one-link-exp} shows the optimal cost function, which
indicates the minimum total cost starting from each state, and
the optimal control policy, which indicates the optimal control for each state.
Dynamic Programming requires storing the optimal control over an
$n$-dimensional grid of the entire state space or in the neighborhood
of the nominal path.
The computation and even the storage of the optimal control becomes
difficult when the dimension is high, which called ``the curse of dimensionality''.
By contrast, if we have an optimal trajectory and local models of the
optimal control policy along it,
then we can get approximations to the optimal control in its neighborhood.
</p>
</div>
</div>
</div>



<div id="outline-container-orgd779ec9" class="outline-2">
<h2 id="orgd779ec9">Introduction of Optimal Control</h2>
<div class="outline-text-2" id="text-orgd779ec9">
</div>
<div id="outline-container-org0427c37" class="outline-3">
<h3 id="org0427c37">Infinite Time Horizon Optimal Control</h3>
<div class="outline-text-3" id="text-org0427c37">
<p>
\[
   \min J  = \sum_{k=0}^{\infty} L(x_k, u_k)
   \]
</p>

<p>
subject to
\[  x_{k+1} = F(x_k, u_k) \]
\[  x_0  = x(0) \]
\[ \ldots  \]
</p>


<ul class="org-ul">
<li>variables: state and control trajectories \(x_0, x_1, \ldots \in \mathrm{R}^N\), \(u_0, u_1, \ldots \in \mathbf{R}^M\)</li>
</ul>
</div>
</div>

<div id="outline-container-org16c0f27" class="outline-3">
<h3 id="org16c0f27">Solution via Dynamic Programming</h3>
<div class="outline-text-3" id="text-org16c0f27">
<ul class="org-ul">
<li>(Bellman)  value function \(V(x)\) is optimal value of control problem as a function of state x.
\[
     V(x_0) = \min \sum_{k=0}^{\infty} L(x_k, u_k)
     \]</li>
<li>\(V\) satisfies Bellman or dynamic programming equation
\[
     V(x_k) = \min_{u_k} \{L(x_k, u_k) + V(x_{k+1})\}
     \]</li>
<li>Optimal control variable is given by
\[
     u^* = arg \min_{u_k} \{L(x, u) + V(F(x, u))\}
     \]
NOTE: the optimal control has 'state feedback form' that \(u_k = \pi(x_k)\)</li>

<li>For a time-invariant linear system, there is analytical solution for \(V(x)\) (solution of Algebraic Riccati Equation)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org74aa5a7" class="outline-2">
<h2 id="org74aa5a7">Introduction of Model Predictive Control</h2>
<div class="outline-text-2" id="text-org74aa5a7">
</div>
<div id="outline-container-org78034a2" class="outline-3">
<h3 id="org78034a2">Model Predictive Control (MPC)</h3>
<div class="outline-text-3" id="text-org78034a2">
<p>
\[
    \min J = \sum_{k=0}^{k=N} L(x_k, u_k)
    \]
</p>

<p>
subject to
\[ x_{k+1} = F(x_k, u_k) \]
\[ x_0 = x(0) \]
\[ x \in X \]
\[ u \in U \]
\[ \ldots \]
with variables \(x_1, \ldots, x_N, u_0, \ldots, u_{N-1}\)
</p>

<ul class="org-ul">
<li>we take \(u = u_0\)</li>
<li>this also gives a state feedback control \(u_k = \pi_{mpc}(x_k)\)</li>
</ul>
</div>
</div>

<div id="outline-container-orge7ad4e2" class="outline-3">
<h3 id="orge7ad4e2">Variation on MPC</h3>
<div class="outline-text-3" id="text-orge7ad4e2">
<ul class="org-ul">
<li>Add final state cost \(\hat{V}(x_f)\) such that
\[
    \min \{ \sum_{k=0}^{k=N} L(x_k, u_k) + \hat{V}(x_{N+1}) \}
    \]</li>

<li>if \(\hat{V} = V\), MPC gives optimal control</li>

<li><p>
Why?
</p>

<p>
Recall Bellman's Principle of Optimality,
\[ \min \sum_{k=0}^{\infty} L(x_k, u_k) = \min \{ \sum_{k=0}^{N}L(x_k, u_k) + \sum_{k=N+1}^{\infty} L(x_k, u_k) \} \]
then we have
\[ \min \sum_{k=0}^{\infty} L(x_k, u_k) = \min \{ \sum_{k=0}^{N}L(x_k, u_k) + V(x_f) \} \]
where \(x_f = x_{N+1}\)
</p></li>
</ul>
</div>
</div>


<div id="outline-container-orgdc2f897" class="outline-3">
<h3 id="orgdc2f897">Variation on MPC (Continuation)</h3>
<div class="outline-text-3" id="text-orgdc2f897">
<ul class="org-ul">
<li>what's the benefit?
<ul class="org-ul">
<li><p>
\color{red} If we have a good approximation to \(V\), MPC's time horizon can be reduced by a lot.
</p>

<p>
NOTE: the computation benefit is at the cost of ignoring future dynamic changes.
</p></li>
</ul></li>

<li>A MPC with a short time horizon can give almost the same result as a MPC with a long time horizon,
which means we can save a lot of computation!</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org4d57b19" class="outline-2">
<h2 id="org4d57b19">CHOMP (Co-variant Hamiltonian Optimization for Motion Planning)</h2>
<div class="outline-text-2" id="text-org4d57b19">
</div>
<div id="outline-container-orgbb984bb" class="outline-3">
<h3 id="orgbb984bb">Remarks</h3>
<div class="outline-text-3" id="text-orgbb984bb">
<ul class="org-ul">
<li>Suited for gradient-cheap application, where gradient can be computed inexpensively</li>
<li>Trajectory optimization is invariant to parameterization
<ul class="org-ul">
<li>trajectory: a mapping from time t to configuration q</li>
<li>objective function: a function of trajectory (functional)</li>
</ul></li>
<li>Compared with sampling-based method, optimal control, and trajectory optimization (DDP and iLQR)</li>
<li><p>
The meaning of the name: Co-variant gradient and Hamiltonian Monte Carlo
</p>

<p>
<a href="https://personalrobotics.ri.cmu.edu/files/courses/papers/CHOMP12-ijrr.pdf">https://personalrobotics.ri.cmu.edu/files/courses/papers/CHOMP12-ijrr.pdf</a>
</p></li>
</ul>
</div>
</div>

<div id="outline-container-org6ba2343" class="outline-3">
<h3 id="org6ba2343">Problem formulation</h3>
<div class="outline-text-3" id="text-org6ba2343">
<p>
\[ \xi(t)^* = \arg \min_{\xi} U(\xi(t)) \]
where  \(U (\xi(t)) = F_{smooth}(\xi(t)) + \lambda F_{obs}(\xi(t))\), and
</p>

<p>
The smoothness objective:
\[\
    F_{smooth}(\xi(t)) = \frac{1}{2} \int_{0}^{1} ||\frac{d}{dt} \xi(t) ||^2 dt
    \]
</p>

<p>
The obstacle objective:
\[
    F_{obs}(\xi(t)) = \int_{0}^{1} \int_{B} c(x(\xi(t), u)) || \frac{d}{dt} x(\xi(t), u)|| du dt
    \]
</p>
</div>
</div>

<div id="outline-container-orgab6d67e" class="outline-3">
<h3 id="orgab6d67e">Functional gradient</h3>
<div class="outline-text-3" id="text-orgab6d67e">
<ul class="org-ul">
<li>The functional gradient \(\nabla U\) is the perturbation \(\phi\)
that maximizes \(U[\xi + \epsilon \phi]\) as \(\epsilon \to 0\).
Choosing a traditional Euclidean norm \(||\xi||^2 = \int \xi(t)^2
     dt\), for any differential objective of form \(F(\xi) = \int L(t,
     \xi, \xi') dt\), the Euclidean functional gradient: \[ \nabla
     F(\xi) = \frac{\partial L}{\partial \xi} - \frac{d}{dt}
     \frac{\partial L}{\partial \dot{\xi}} \]</li>

<li>Euler-Lagrange equation
\[
     \frac{\partial L}{\partial q} - \frac{d}{dt} \frac{\partial L}{\partial \dot{q}} = 0
     \]
Euler-Lagrange equation is the stationary condition for:
\[
     \min_{q(t)} S(q) = \int_{a}^{b} L(t, q, \dot{q}) dt
     \]</li>
</ul>
</div>
</div>

<div id="outline-container-orgc74b8d3" class="outline-3">
<h3 id="orgc74b8d3">Functional gradient</h3>
<div class="outline-text-3" id="text-orgc74b8d3">
<ul class="org-ul">
<li>The functional gradient of the smoothness objective:</li>
</ul>
<p>
\[
    \nabla F_{smooth} = - \ddot{\xi}
    \]
</p>

<ul class="org-ul">
<li>The functional gradient of the obstacle objective</li>
</ul>
<p>
\[
    \nabla F_{obs} = \int_{B} J^T ||x'|| \big( (I - \hat{x}'\hat{x}^T)\nabla{c} -ck\big) du
    \]
where \(k = \frac{(I - \hat{x}'\hat{x}'^T) x''}{||x'||^2}\) is the curvature vector along the workspace trajectory.
</p>

<p>
The matrix \((I - \hat{x}'\hat{x}'^T)\) is a projection matrix that projects workspace gradients orthogonally to the trajectorys direction of motion.
</p>
</div>
</div>

<div id="outline-container-org54f3a29" class="outline-3">
<h3 id="org54f3a29">Functionals and Co-variant Gradients for Trajectory Optimization</h3>
<div class="outline-text-3" id="text-org54f3a29">
<ul class="org-ul">
<li>To removing the dependence on the parametrization (finite difference),
defining a norm on trajectory perturbations to depend solely on the
dynamical quantities of the trajectory:
\[ ||\xi||_A := \int \sum_{n=1}^{k} \alpha_n (D^n \xi(t))^2 dt \]
where \(D\) is differential operator, \(A = D^T D\).</li>
</ul>


<p>
Invariant norm \(||\xi(t)||_{A} := <\xi, A\xi> = \int (D\xi(t))^2 dt\)
</p>

<ul class="org-ul">
<li>Functional gradients based on the invariant norm:
\[ \bar{\nabla}_{A} U(\xi) := A^{-1} \bar{\nabla} U(\xi) \]</li>
</ul>
</div>
</div>

<div id="outline-container-orga36400f" class="outline-3">
<h3 id="orga36400f">Waypoint Trajectory Parameterization</h3>
<div class="outline-text-3" id="text-orga36400f">
<p>
refer to the paper
</p>
</div>
</div>

<div id="outline-container-orgc95d3e1" class="outline-3">
<h3 id="orgc95d3e1">Update rule:</h3>
<div class="outline-text-3" id="text-orgc95d3e1">
<p>
\[ \xi_{i+1} = \arg \min_{\xi} U(\xi_i) + (\xi - \xi_i)^T \nabla U(\xi_i) + \frac{\eta}{2} || \xi - \xi_i||_M \]
</p>

<p>
This update rule is covariant in the sense that the change to the trajectory that results from the update
is a function only of the trajectory itself, and not the particular representation used (e.g. waypoint
based), at least in the limit of small step size and fine discretization.
</p>

<p>
The overall CHOMP objective function is strongly convex  that is, it can be
lower-bounded over the entire region by a quadratic with curvature A
\[ \xi_{i+1} = \xi_{i} - \eta A^{-1} \nabla U(\xi_{i}) \]
</p>
</div>
</div>

<div id="outline-container-org2f06655" class="outline-3">
<h3 id="org2f06655">Hamiltonian Monte Carlo</h3>
<div class="outline-text-3" id="text-org2f06655">
<p>
refer: <a href="http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html">http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html</a>
</p>

<ul class="org-ul">
<li>Intuition
\[ p(\xi, \alpha) \propto exp(-\alpha U(\xi)) \]</li>

<li><p>
define a Hamilton function as
\[ H(\xi, \gamma) := U(\xi) + K(\gamma)  \]
where \(K(\gamma) = \frac{1}{2} \gamma'\gamma\).
</p>

<p>
\[ p(\xi, \gamma) \propto exp(-H(\xi, \gamma)) \]
</p>

<p>
The system dynamics (Hamilton equation):
\[ \frac{d\xi}{dt} = \gamma \]
\[ \frac{\gamma}{dt} = -\nabla U(\xi) \]
</p>

<p>
The full numerically robust Hamiltonian Monte Carlo. refer to the paper
</p></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org16d54f6" class="outline-2">
<h2 id="org16d54f6">Optimal Smoothing and Its Application</h2>
<div class="outline-text-2" id="text-org16d54f6">
</div>
<div id="outline-container-orga709b15" class="outline-3">
<h3 id="orga709b15">Optimal Smoothing Background</h3>
<div class="outline-text-3" id="text-orga709b15">
</div>
<div id="outline-container-orgd5e00db" class="outline-4">
<h4 id="orgd5e00db">Optimal smoothing</h4>
<div class="outline-text-4" id="text-orgd5e00db">
<p>
To quantify the uncertainty associated with an unobserved variable, \(X\), given some information pertaining to that
variable, \(Y\). In a sequential context, this relates to the calculation of the posterior density, \(p(x_{1:T}|
   y_{1:T})\), where \(x_{1:T} := {x_1, \ldots, x_T }\), \(y_{1:T} := {y_1, \ldots, y_T}\), are the state to estimate and the
observation processes, and the discrete time index \(t\in \mathbb{N}^+\).
</p>

<p>
To facilitate sequential estimation, a first order Markovian state space model is
often assumed and defined by its initial probability \(X_1 \sim \mu(\cdot)\) and
for \(t > 0\):
</p>

<p>
\[ X_t| (X_{t-1} = x_{t-1}) \sim f(\cdot| x_{t-1});   \]
</p>

<p>
Furthermore, the observations are conditional upon the state and are
assumed to be statistically independent and distributed according to:
</p>

<p>
\[ Y_t | (X_t=x_t) \sim g(\cdot| x_t), \]
</p>

<p>
where we assume that \(\mu(\cdot)\), \(f(\cdot| x_{t-1})\) and \(g(\cdot| x_t)\) are
probability densities with respect to some dominating measure.
</p>

<p>
The joint posterior density \(p(x_{1:T}| y_{1:T})\) is simply given by:
\[ p(x_{1:t}| y_{1:t}) \propto \mu(x_1)\prod_{k=2}^{t}f(x_k | x_{k-1}) \prod_{k=1}^{t} g(y_k | x_k) \]
</p>

<p>
Given the empirical observations \(y_{1:T}\), the optimal fix-interval smoother
tries to find a sequence of the state variables that gives the maximum a
posterior probability (MAP):
</p>

\begin{equation}
\max_{x_{1:T}} \mu(x_1)\prod_{k=2}^{t}f(x_k | x_{k-1}) \prod_{k=1}^{t} g(y_k | x_k)
\end{equation}

<p>
subjective to
\[ X_t| (X_{t-1} = x_{t-1}) \sim f(\cdot| x_{t-1});   \]
\[ Y_t | (X_t=x_t) \sim g(\cdot| x_t), \]
\[ X_1 \sim \mu(\cdot) \]
</p>
</div>
</div>
<div id="outline-container-orgdcfccbb" class="outline-4">
<h4 id="orgdcfccbb">Optimal linear quadratic smoother</h4>
<div class="outline-text-4" id="text-orgdcfccbb">
<p>
The process model:
\[ x_{k} = A_k x_{k-1}  + w_{k-1}  \]
The observation model:
\[ y_k = H_k x_k + v_k\]
</p>

<p>
where \(w_k\) is the process noise which is assumed to be drawn from a zero
mean multivariate normal distribution, \(w_k \sim \mathcal{N}(0, Q_k)\), and
\(v_k\) is the observation noise which is assumed to be zero mean Gaussian
white noise with covariance \(R_k\), \(v_k \sim \mathcal{N}(0, R_k)\). The
covariances of the two noise models are assumed stationary over time and are
given by: \[ Q = E(w_k, w_k) \] \[ R = E(v_k, v_k) \]
</p>

<p>
Recall Eq. \ref{eq:1}, for the linear process model and observation model, we have:
\[ \mu(x_1) = |2 \pi P|^{-2} \exp(-(x_1 - \hat{x}_1)^{\top} P^{-1} (x_1 - \hat{x}_1)) \]
\[ f(x_k| x_{k-1}) = |2 \pi Q|^{-2} \exp(-(x_k - A_{k-1} x_{k-1})^{\top} Q^{-1}  (x_k - A_{k-1} x_{k-1}) ) \]
\[ g(y_k| x_k) = |2 \pi R|^{-2} \exp(-(y_k - H_k x_k)^{\top} R^{-1} (y_k - H_k x_k)) \]
where \(\hat{x}_1\) and \(P\) are the initial state's prior estimation and covariance.
</p>

<p>
Then, the fix-interval optimal smoother problem can be reformulated as:
</p>
\begin{equation}
\min_{x_{1:T}} \frac{1}{2} \sum_{k=1}^{N} v_k^{\top} R^{-1} v_k +
\frac{1}{2} \sum_{k=2}^{T} w_k^{\top} Q^{-1} w_k +
\frac{1}{2} (x_1 -\hat{x}_1)^{\top} P^{-1} (x_1 - \hat{x}_1)
\end{equation}
<p>
where
\[ v_k = y_k - H_k x_k \]
\[w_k = x_k - A_{k-1} x_{k-1} \]
</p>

<p>
In practice, Eq. \ref{eq:2} can be solved by batch least square (BLS)
estimation methods or sequential recursive estimation methods, such as Fraser
and Potter (1969), Rauch, Tung, and Striebel (1965), and etc.
</p>
</div>
</div>
</div>
</div>
</div>
</body>
</html>
